from transformers import AutoTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
import warnings

warnings.filterwarnings('ignore')

# 1. ë¼ë²¨ ëª©ë¡ ì •ì˜ (ê¸°ì¡´ 44ê°œ + í—ˆë¬´ ì¶”ê°€)
labels = {
    0: 'ë¶ˆí‰/ë¶ˆë§Œ', 1: 'í™˜ì˜/í˜¸ì˜', 2: 'ê°ë™/ê°íƒ„', 3: 'ì§€ê¸‹ì§€ê¸‹', 4: 'ê³ ë§ˆì›€', 5: 'ìŠ¬í””', 6: 'í™”ë‚¨/ë¶„ë…¸',
    7: 'ì¡´ê²½', 8: 'ê¸°ëŒ€ê°', 9: 'ìš°ì­ëŒ/ë¬´ì‹œí•¨', 10: 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 11: 'ë¹„ì¥í•¨', 12: 'ì˜ì‹¬/ë¶ˆì‹ ',
    13: 'ë¿Œë“¯í•¨', 14: 'í¸ì•ˆ/ì¾Œì ', 15: 'ì‹ ê¸°í•¨/ê´€ì‹¬', 16: 'ì•„ê»´ì£¼ëŠ”', 17: 'ë¶€ë„ëŸ¬ì›€', 18: 'ê³µí¬/ë¬´ì„œì›€',
    19: 'ì ˆë§', 20: 'í•œì‹¬í•¨', 21: 'ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€', 22: 'ì§œì¦', 23: 'ì–´ì´ì—†ìŒ', 24: 'ì—†ìŒ', 25: 'íŒ¨ë°°/ìê¸°í˜ì˜¤',
    26: 'ê·€ì°®ìŒ', 27: 'í˜ë“¦/ì§€ì¹¨', 28: 'ì¦ê±°ì›€/ì‹ ë‚¨', 29: 'ê¹¨ë‹¬ìŒ', 30: 'ì£„ì±…ê°', 31: 'ì¦ì˜¤/í˜ì˜¤',
    32: 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)', 33: 'ë‹¹í™©/ë‚œì²˜', 34: 'ê²½ì•…', 35: 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´', 36: 'ì„œëŸ¬ì›€',
    37: 'ì¬ë¯¸ì—†ìŒ', 38: 'ë¶ˆìŒí•¨/ì—°ë¯¼', 39: 'ë†€ëŒ', 40: 'í–‰ë³µ', 41: 'ë¶ˆì•ˆ/ê±±ì •', 42: 'ê¸°ì¨',
    43: 'ì•ˆì‹¬/ì‹ ë¢°', 44: 'í—ˆë¬´'
}
label2id = {v: k for k, v in labels.items()}
id2label = labels

print(f"ğŸ“Š ì´ ë¼ë²¨ ìˆ˜: {len(labels)}ê°œ")
print(f"ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ ë¼ë²¨: {labels[44]} (ID: 44)")

# 2. ë°ì´í„° ë¡œë”©
with open("finetune.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

print(f"ğŸ“„ ìƒˆ ë¼ë²¨ í•™ìŠµ ë°ì´í„°: {len(raw_data)}ê°œ")
dataset = Dataset.from_list(raw_data)

# 3. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”©
model_name = "searle-j/kote_for_easygoing_people"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (44ê°œ ë¼ë²¨)
print("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = ElectraForSequenceClassification.from_pretrained(model_name)
print(f"ê¸°ì¡´ ëª¨ë¸ ë¼ë²¨ ìˆ˜: {model.config.num_labels}")

# 4. ëª¨ë¸ í™•ì¥: 44ê°œ â†’ 45ê°œ ë¼ë²¨
print("ğŸ”§ ëª¨ë¸ ì¶œë ¥ì¸µ í™•ì¥ ì¤‘...")

# ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë³´ì¡´
old_classifier = model.classifier.out_proj
old_weight = old_classifier.weight.data
old_bias = old_classifier.bias.data

# ìƒˆë¡œìš´ ë¶„ë¥˜ì¸µ ìƒì„± (45ê°œ ë¼ë²¨)
new_classifier = nn.Linear(model.config.hidden_size, len(labels))

# ê¸°ì¡´ 44ê°œ ë¼ë²¨ì˜ ê°€ì¤‘ì¹˜ ë³µì‚¬
with torch.no_grad():
    new_classifier.weight.data[:44] = old_weight
    new_classifier.bias.data[:44] = old_bias

    # ìƒˆë¡œìš´ ë¼ë²¨(í—ˆë¬´)ì˜ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    # ìœ ì‚¬í•œ ê°ì •ë“¤(ìŠ¬í””, ì ˆë§, í—ˆë¬´)ì˜ í‰ê· ìœ¼ë¡œ ì´ˆê¸°í™”
    similar_emotions = [5, 19, 25, 36]  # ìŠ¬í””, ì ˆë§, íŒ¨ë°°/ìê¸°í˜ì˜¤, ì„œëŸ¬ì›€
    similar_weights = old_weight[similar_emotions].mean(dim=0)
    similar_bias = old_bias[similar_emotions].mean()

    new_classifier.weight.data[44] = similar_weights
    new_classifier.bias.data[44] = similar_bias

# ëª¨ë¸ ì—…ë°ì´íŠ¸
model.classifier.out_proj = new_classifier
model.config.num_labels = len(labels)
model.config.label2id = label2id
model.config.id2label = id2label
model.config.problem_type = "single_label_classification"

print(f"âœ… ëª¨ë¸ í™•ì¥ ì™„ë£Œ: {model.config.num_labels}ê°œ ë¼ë²¨")


# 5. ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess(examples):
    return tokenizer(examples["sentence"], truncation=True, padding="max_length", max_length=128)


def encode_label(example):
    example["label"] = label2id[example["label"]]
    return example


dataset = dataset.map(preprocess, batched=True)
dataset = dataset.map(encode_label)


# 6. Few-shot Learningì„ ìœ„í•œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ
class FewShotTrainer(Trainer):
    def __init__(self, *args, old_model_state=None, regularization_weight=0.1, **kwargs):
        super().__init__(*args, **kwargs)
        self.old_model_state = old_model_state
        self.regularization_weight = regularization_weight

    def compute_loss(self, model, inputs, return_outputs=False):
        """
        íŒŒë©¸ì  ë§ê° ë°©ì§€ë¥¼ ìœ„í•œ ì •ê·œí™”ëœ ì†ì‹¤ í•¨ìˆ˜
        """
        # ê¸°ë³¸ ë¶„ë¥˜ ì†ì‹¤
        outputs = model(**inputs)
        loss = outputs.loss

        # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ì˜ ì°¨ì´ì— ëŒ€í•œ ì •ê·œí™” í•­ ì¶”ê°€
        if self.old_model_state is not None:
            reg_loss = 0
            for name, param in model.named_parameters():
                if name in self.old_model_state and 'classifier.weight' in name:
                    # ê¸°ì¡´ 44ê°œ ë¼ë²¨ì˜ ê°€ì¤‘ì¹˜ë§Œ ì •ê·œí™”
                    old_weight = self.old_model_state[name][:44]
                    new_weight = param[:44]
                    reg_loss += F.mse_loss(new_weight, old_weight)

            loss += self.regularization_weight * reg_loss

        return (loss, outputs) if return_outputs else loss


# 7. ê¸°ì¡´ ëª¨ë¸ ìƒíƒœ ì €ì¥ (ì •ê·œí™”ìš©)
old_model_state = {}
for name, param in model.named_parameters():
    old_model_state[name] = param.data.clone()

# 8. í•™ìŠµ ì„¤ì • (Few-shot Learningì— ìµœì í™”)
training_args = TrainingArguments(
    output_dir="./results_void",
    evaluation_strategy="no",
    logging_steps=5,
    learning_rate=1e-5,  # ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ
    per_device_train_batch_size=4,  # ì‘ì€ ë°°ì¹˜ í¬ê¸°
    num_train_epochs=10,  # ë” ë§ì€ ì—í¬í¬
    weight_decay=0.01,
    logging_dir="./logs",
    save_steps=50,
    warmup_steps=10,  # Warmup ì¶”ê°€
    gradient_accumulation_steps=2,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
    dataloader_drop_last=False,
    remove_unused_columns=False,
)

# 9. Few-shot íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = FewShotTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    old_model_state=old_model_state,
    regularization_weight=0.1  # ì •ê·œí™” ê°€ì¤‘ì¹˜
)

# 10. í•™ìŠµ ì „ ëª¨ë¸ ìƒíƒœ ì¶œë ¥
print("\nğŸ” í•™ìŠµ ì „ ëª¨ë¸ ì •ë³´:")
print(f"ğŸ”¢ ì¶œë ¥ì¸µ: {model.classifier}")
print(f"ğŸ¯ Configì˜ ë¼ë²¨ ìˆ˜: {model.config.num_labels}")
print(f"ğŸ“Š ìƒˆ ë¼ë²¨ ì´ˆê¸° ê°€ì¤‘ì¹˜ norm: {model.classifier.out_proj.weight.data[44].norm():.4f}")

# 11. íŒŒì¸íŠœë‹ ì‹œì‘
print("\nğŸš€ Few-shot Learning ê¸°ë°˜ íŒŒì¸íŠœë‹ ì‹œì‘...")
trainer.train()

# 12. í•™ìŠµ í›„ ê²€ì¦
print("\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ê²€ì¦ ì¤‘...")

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
test_sentences = [
    "ëª¨ë“  ê²Œ ë¬´ì˜ë¯¸í•˜ê²Œ ëŠê»´ì§„ë‹¤.",  # í—ˆë¬´
    "ì •ë§ ê¸°ì˜ê³  í–‰ë³µí•˜ë‹¤.",  # ê¸°ì¨
    "ë„ˆë¬´ í™”ê°€ ë‚œë‹¤.",  # í™”ë‚¨
    "ì‚¶ì— ì˜ë¯¸ê°€ ìˆì„ê¹Œ ì‹¶ë‹¤."  # í—ˆë¬´
]

model.eval()
with torch.no_grad():
    for sentence in test_sentences:
        inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True, max_length=128)
        outputs = model(**inputs)
        predicted_id = torch.argmax(outputs.logits, dim=-1).item()
        confidence = torch.softmax(outputs.logits, dim=-1).max().item()

        print(f"ğŸ“ ë¬¸ì¥: '{sentence}'")
        print(f"ğŸ¯ ì˜ˆì¸¡: {labels[predicted_id]} (ì‹ ë¢°ë„: {confidence:.3f})")
        print()

# 13. ëª¨ë¸ ì €ì¥
save_path = "./custom_kote_with_void"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"ğŸ’¾ íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ·ï¸ ì´ {len(labels)}ê°œ ê°ì • ë¶„ë¥˜ ê°€ëŠ¥")
print(f"ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ ê°ì •: '{labels[44]}'")

# 14. ìµœì¢… ëª¨ë¸ ì •ë³´ ì¶œë ¥
print("\nğŸ“‹ ìµœì¢… ëª¨ë¸ ì •ë³´:")
print(f"- ê¸°ì¡´ ë¼ë²¨: 0~43 (44ê°œ)")
print(f"- ìƒˆ ë¼ë²¨: 44 (í—ˆë¬´)")
print(f"- ì´ ë¼ë²¨ ìˆ˜: {len(labels)}ê°œ")
print(f"- í•™ìŠµ ë°ì´í„°: {len(raw_data)}ê°œ (í—ˆë¬´ ê°ì •)")
print(f"- ì €ì¥ ìœ„ì¹˜: {save_path}")
