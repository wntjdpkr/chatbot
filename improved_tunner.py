from transformers import AutoTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
import warnings

warnings.filterwarnings('ignore')

# ğŸ”§ GPU ì„¤ì •
print("ğŸ” GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ì¤‘...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ“± ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")

if torch.cuda.is_available():
    print(f"ğŸ® GPU ì •ë³´:")
    print(f"  - GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    print(f"  - í˜„ì¬ GPU: {torch.cuda.current_device()}")
    print(f"  - GPU ì´ë¦„: {torch.cuda.get_device_name()}")
    print(f"  - GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f}GB")
else:
    print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# 1. ë¼ë²¨ ëª©ë¡ ì •ì˜ (ê¸°ì¡´ 44ê°œ + í—ˆë¬´ ì¶”ê°€)
labels = {
    0: 'ë¶ˆí‰/ë¶ˆë§Œ', 1: 'í™˜ì˜/í˜¸ì˜', 2: 'ê°ë™/ê°íƒ„', 3: 'ì§€ê¸‹ì§€ê¸‹', 4: 'ê³ ë§ˆì›€', 5: 'ìŠ¬í””', 6: 'í™”ë‚¨/ë¶„ë…¸',
    7: 'ì¡´ê²½', 8: 'ê¸°ëŒ€ê°', 9: 'ìš°ì­ëŒ/ë¬´ì‹œí•¨', 10: 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 11: 'ë¹„ì¥í•¨', 12: 'ì˜ì‹¬/ë¶ˆì‹ ',
    13: 'ë¿Œë“¯í•¨', 14: 'í¸ì•ˆ/ì¾Œì ', 15: 'ì‹ ê¸°í•¨/ê´€ì‹¬', 16: 'ì•„ê»´ì£¼ëŠ”', 17: 'ë¶€ë„ëŸ¬ì›€', 18: 'ê³µí¬/ë¬´ì„œì›€',
    19: 'ì ˆë§', 20: 'í•œì‹¬í•¨', 21: 'ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€', 22: 'ì§œì¦', 23: 'ì–´ì´ì—†ìŒ', 24: 'ì—†ìŒ', 25: 'íŒ¨ë°°/ìê¸°í˜ì˜¤',
    26: 'ê·€ì°®ìŒ', 27: 'í˜ë“¦/ì§€ì¹¨', 28: 'ì¦ê±°ì›€/ì‹ ë‚¨', 29: 'ê¹¨ë‹¬ìŒ', 30: 'ì£„ì±…ê°', 31: 'ì¦ì˜¤/í˜ì˜¤',
    32: 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)', 33: 'ë‹¹í™©/ë‚œì²˜', 34: 'ê²½ì•…', 35: 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´', 36: 'ì„œëŸ¬ì›€',
    37: 'ì¬ë¯¸ì—†ìŒ', 38: 'ë¶ˆìŒí•¨/ì—°ë¯¼', 39: 'ë†€ëŒ', 40: 'í–‰ë³µ', 41: 'ë¶ˆì•ˆ/ê±±ì •', 42: 'ê¸°ì¨',
    43: 'ì•ˆì‹¬/ì‹ ë¢°', 44: 'í—ˆë¬´'
}
label2id = {v: k for k, v in labels.items()}
id2label = labels

print(f"ğŸ“Š ì´ ë¼ë²¨ ìˆ˜: {len(labels)}ê°œ")
print(f"ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ ë¼ë²¨: {labels[44]} (ID: 44)")

# 2. ë°ì´í„° ë¡œë”©
with open("finetune.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

print(f"ğŸ“„ ìƒˆ ë¼ë²¨ í•™ìŠµ ë°ì´í„°: {len(raw_data)}ê°œ")
dataset = Dataset.from_list(raw_data)

# 3. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”©
model_name = "searle-j/kote_for_easygoing_people"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (44ê°œ ë¼ë²¨)
print("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = ElectraForSequenceClassification.from_pretrained(model_name)
print(f"ê¸°ì¡´ ëª¨ë¸ ë¼ë²¨ ìˆ˜: {model.config.num_labels}")

# ğŸ® ëª¨ë¸ì„ GPUë¡œ ì´ë™
print(f"ğŸš€ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì¤‘...")
model = model.to(device)
print(f"âœ… ëª¨ë¸ì´ {device}ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ElectraClassificationHead êµ¬ì¡° í™•ì¸
print("ğŸ” ë¶„ë¥˜ì¸µ êµ¬ì¡° í™•ì¸:")
print(f"Classifier type: {type(model.classifier)}")
for name, module in model.classifier.named_children():
    print(f"  - {name}: {module}")

# 4. ëª¨ë¸ í™•ì¥: 44ê°œ â†’ 45ê°œ ë¼ë²¨
print("ğŸ”§ ëª¨ë¸ ì¶œë ¥ì¸µ í™•ì¥ ì¤‘...")

# ElectraClassificationHeadì—ì„œ ì‹¤ì œ ì¶œë ¥ì¸µ ì ‘ê·¼
# ElectraClassificationHead êµ¬ì¡°: dense -> dropout -> out_proj
old_classifier = model.classifier
if hasattr(old_classifier, 'out_proj'):
    # out_projê°€ ì‹¤ì œ ë¶„ë¥˜ë¥¼ ë‹´ë‹¹í•˜ëŠ” ë ˆì´ì–´
    old_out_proj = old_classifier.out_proj
    old_weight = old_out_proj.weight.data  # [44, hidden_size]
    old_bias = old_out_proj.bias.data  # [44]
    hidden_size = old_weight.shape[1]
    print(f"ê¸°ì¡´ out_proj í¬ê¸°: {old_weight.shape}")
elif hasattr(old_classifier, 'dense') and hasattr(old_classifier, 'out_proj'):
    # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” dense -> out_proj êµ¬ì¡°
    old_out_proj = old_classifier.out_proj
    old_weight = old_out_proj.weight.data
    old_bias = old_out_proj.bias.data
    hidden_size = old_weight.shape[1]
else:
    # ë‹¤ë¥¸ êµ¬ì¡°ì¼ ê²½ìš° ì§ì ‘ í™•ì¸
    print("âŒ ì˜ˆìƒì¹˜ ëª»í•œ ë¶„ë¥˜ì¸µ êµ¬ì¡°ì…ë‹ˆë‹¤.")
    print("ë¶„ë¥˜ì¸µ ë‚´ë¶€ êµ¬ì¡°:")
    for name, param in old_classifier.named_parameters():
        print(f"  {name}: {param.shape}")

    # ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©
    last_layer = None
    for name, module in old_classifier.named_modules():
        if isinstance(module, nn.Linear):
            last_layer = module

    if last_layer is not None:
        old_weight = last_layer.weight.data
        old_bias = last_layer.bias.data
        hidden_size = old_weight.shape[1]
        print(f"âœ… ë§ˆì§€ë§‰ Linear ë ˆì´ì–´ ë°œê²¬: {old_weight.shape}")
    else:
        raise ValueError("ë¶„ë¥˜ì¸µì—ì„œ Linear ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ìƒˆë¡œìš´ ì¶œë ¥ì¸µ ìƒì„± (45ê°œ ë¼ë²¨) - GPUì—ì„œ ìƒì„±
print(f"ğŸ”§ ìƒˆë¡œìš´ ì¶œë ¥ì¸µì„ {device}ì—ì„œ ìƒì„± ì¤‘...")
new_out_proj = nn.Linear(hidden_size, len(labels)).to(device)

# ê¸°ì¡´ 44ê°œ ë¼ë²¨ì˜ ê°€ì¤‘ì¹˜ ë³µì‚¬
with torch.no_grad():
    new_out_proj.weight.data[:44] = old_weight
    new_out_proj.bias.data[:44] = old_bias

    # ìƒˆë¡œìš´ ë¼ë²¨(í—ˆë¬´)ì˜ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    # ìœ ì‚¬í•œ ê°ì •ë“¤(ìŠ¬í””, ì ˆë§, í—ˆë¬´)ì˜ í‰ê· ìœ¼ë¡œ ì´ˆê¸°í™”
    similar_emotions = [5, 19, 25, 36]  # ìŠ¬í””, ì ˆë§, íŒ¨ë°°/ìê¸°í˜ì˜¤, ì„œëŸ¬ì›€
    similar_weights = old_weight[similar_emotions].mean(dim=0)
    similar_bias = old_bias[similar_emotions].mean()

    boost_factor = 2.0  # ì˜ˆì‹œê°’, 2.0ê¹Œì§€ë„ ì‹¤í—˜ ê°€ëŠ¥
    new_out_proj.weight.data[44] = similar_weights * boost_factor
    new_out_proj.bias.data[44] = similar_bias * boost_factor

# ElectraClassificationHeadì˜ out_projë§Œ êµì²´
if hasattr(old_classifier, 'out_proj'):
    old_classifier.out_proj = new_out_proj
else:
    # ì „ì²´ ë¶„ë¥˜ì¸µì„ ìƒˆë¡œ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ê²½ìš°
    from transformers.models.electra.modeling_electra import ElectraClassificationHead

    # ìƒˆë¡œìš´ ElectraClassificationHead ìƒì„±
    config = model.config
    config.num_labels = len(labels)

    new_classifier = ElectraClassificationHead(config).to(device)
    # ê¸°ì¡´ dense ë ˆì´ì–´ëŠ” ìœ ì§€í•˜ê³  out_projë§Œ êµì²´
    if hasattr(old_classifier, 'dense'):
        new_classifier.dense = old_classifier.dense
    if hasattr(old_classifier, 'dropout'):
        new_classifier.dropout = old_classifier.dropout
    new_classifier.out_proj = new_out_proj

    model.classifier = new_classifier

model.config.num_labels = len(labels)
model.config.label2id = label2id
model.config.id2label = id2label
model.config.problem_type = "single_label_classification"

print(f"âœ… ëª¨ë¸ í™•ì¥ ì™„ë£Œ: {model.config.num_labels}ê°œ ë¼ë²¨")
print(f"ğŸ® ëª¨ë¸ì´ {device}ì—ì„œ ì‹¤í–‰ ì¤‘")

# 5. ë°ì´í„° êµ¬ì¡° í™•ì¸ ë° ì „ì²˜ë¦¬
print("ğŸ” ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸:")
print(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {dataset.column_names}")
print(f"ë°ì´í„°ì…‹ íƒ€ì…: {type(dataset)}")
print(f"ë°ì´í„°ì…‹ ê¸¸ì´: {len(dataset)}")

# ì²« ë²ˆì§¸ ìƒ˜í”Œ ì•ˆì „í•˜ê²Œ í™•ì¸
try:
    first_sample = dataset[0]
    print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ: {first_sample}")
    print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ íƒ€ì…: {type(first_sample)}")
except Exception as e:
    print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ ì ‘ê·¼ ì˜¤ë¥˜: {e}")


# ì „ì²˜ë¦¬ í•¨ìˆ˜ ê°œì„ 
def preprocess_function(examples):
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§•ê³¼ ë¼ë²¨ ì¸ì½”ë”©ì„ ë™ì‹œì— ì²˜ë¦¬
    """
    try:
        # ì…ë ¥ í™•ì¸
        sentences = examples["sentence"]
        labels = examples["label"]

        print(f"ì „ì²˜ë¦¬ ì¤‘ - ë°°ì¹˜ í¬ê¸°: {len(sentences)}")
        print(f"ì²« ë²ˆì§¸ ë¬¸ì¥: {sentences[0] if sentences else 'None'}")
        print(f"ì²« ë²ˆì§¸ ë¼ë²¨: {labels[0] if labels else 'None'}")

        # í† í¬ë‚˜ì´ì§•
        tokenized = tokenizer(
            sentences,
            truncation=True,
            padding="max_length",
            max_length=128,
            return_tensors=None  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        )

        # ë¼ë²¨ ì¸ì½”ë”©
        encoded_labels = []
        for label in labels:
            if label in label2id:
                encoded_labels.append(label2id[label])
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë¼ë²¨: {label}")
                encoded_labels.append(44)  # ê¸°ë³¸ê°’ìœ¼ë¡œ í—ˆë¬´ ì„¤ì •

        tokenized["labels"] = encoded_labels

        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ - ë¼ë²¨ë“¤: {encoded_labels[:3]}...")  # ì²« 3ê°œë§Œ ì¶œë ¥

        return tokenized

    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"Examples íƒ€ì…: {type(examples)}")
        print(f"Examples ë‚´ìš©: {examples}")
        raise e


# ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©
print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
try:
    dataset = dataset.map(
        preprocess_function,
        batched=True,
        batch_size=8,  # ì‘ì€ ë°°ì¹˜ë¡œ ì‹œì‘
        remove_columns=dataset.column_names  # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
    )
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
except Exception as e:
    print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    print("ê°œë³„ ì²˜ë¦¬ë¡œ ì‹œë„...")

    # ê°œë³„ ì²˜ë¦¬ fallback
    processed_data = []
    for i, sample in enumerate(dataset):
        try:
            sentence = sample["sentence"]
            label = sample["label"]

            # í† í¬ë‚˜ì´ì§•
            tokenized = tokenizer(
                sentence,
                truncation=True,
                padding="max_length",
                max_length=128,
                return_tensors=None
            )

            # ë¼ë²¨ ì¸ì½”ë”©
            tokenized["labels"] = label2id[label]
            processed_data.append(tokenized)

            if i < 3:  # ì²« 3ê°œë§Œ ë¡œê·¸
                print(f"ê°œë³„ ì²˜ë¦¬ ì™„ë£Œ {i}: ë¼ë²¨ {label} -> {tokenized['labels']}")

        except Exception as inner_e:
            print(f"ìƒ˜í”Œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {inner_e}")

    # ìƒˆ ë°ì´í„°ì…‹ ìƒì„±
    from datasets import Dataset

    dataset = Dataset.from_list(processed_data)
    print("âœ… ê°œë³„ ì²˜ë¦¬ë¡œ ì „ì²˜ë¦¬ ì™„ë£Œ!")

print(f"ì „ì²˜ë¦¬ í›„ ì»¬ëŸ¼: {dataset.column_names}")
if len(dataset) > 0:
    print(f"ì „ì²˜ë¦¬ í›„ ì²« ë²ˆì§¸ ìƒ˜í”Œ í‚¤: {list(dataset[0].keys())}")

# ë°ì´í„° íƒ€ì… í™•ì¸
sample = dataset[0]
print("ğŸ“Š ë°ì´í„° íƒ€ì… í™•ì¸:")
for key, value in sample.items():
    print(
        f"  {key}: {type(value)} - {value if key == 'labels' else f'ê¸¸ì´ {len(value) if isinstance(value, list) else value}'}")

# ëª‡ ê°œ ìƒ˜í”Œì˜ ë¼ë²¨ í™•ì¸
print("\nğŸ¯ ë¼ë²¨ ë¶„í¬ í™•ì¸:")
# ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ë¼ë²¨ ì ‘ê·¼
try:
    # ë°©ë²• 1: ê°œë³„ ìƒ˜í”Œ ì ‘ê·¼
    labels_list = []
    for i in range(min(10, len(dataset))):
        sample = dataset[i]
        labels_list.append(sample['labels'])
    print(f"ì²« 10ê°œ ìƒ˜í”Œì˜ ë¼ë²¨: {labels_list}")
except Exception as e:
    print(f"ê°œë³„ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

    # ë°©ë²• 2: ì»¬ëŸ¼ ì§ì ‘ ì ‘ê·¼
    try:
        labels_list = dataset['labels'][:10]
        print(f"ì²« 10ê°œ ìƒ˜í”Œì˜ ë¼ë²¨: {labels_list}")
    except Exception as e2:
        print(f"ì»¬ëŸ¼ ì ‘ê·¼ë„ ì‹¤íŒ¨: {e2}")

        # ë°©ë²• 3: ì „ì²´ ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        print("ë°ì´í„°ì…‹ ì „ì²´ ì •ë³´:")
        print(f"ë°ì´í„°ì…‹ íƒ€ì…: {type(dataset)}")
        print(f"ë°ì´í„°ì…‹ ê¸¸ì´: {len(dataset)}")
        print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ íƒ€ì…: {type(dataset[0])}")
        if hasattr(dataset, 'features'):
            print(f"Features: {dataset.features}")

# ëª¨ë“  ë¼ë²¨ì´ 44(í—ˆë¬´)ì¸ì§€ í™•ì¸
try:
    # ë°©ë²• 1: ì»¬ëŸ¼ ì§ì ‘ ì ‘ê·¼
    if hasattr(dataset, 'features') and 'labels' in dataset.features:
        unique_labels = set(dataset['labels'])
        labels_count_44 = sum(1 for label in dataset['labels'] if label == 44)
    else:
        # ë°©ë²• 2: ê°œë³„ ì ‘ê·¼
        all_labels = []
        for i in range(len(dataset)):
            sample = dataset[i]
            all_labels.append(sample['labels'])
        unique_labels = set(all_labels)
        labels_count_44 = sum(1 for label in all_labels if label == 44)

    print(f"ë°ì´í„°ì…‹ì˜ ê³ ìœ  ë¼ë²¨: {unique_labels}")
    print(f"ë¼ë²¨ 44('í—ˆë¬´')ê°€ {labels_count_44}ê°œ ìˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"ë¼ë²¨ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    print("ê°œë³„ ìƒ˜í”Œ 3ê°œ í™•ì¸:")
    for i in range(min(3, len(dataset))):
        print(f"  ìƒ˜í”Œ {i}: {dataset[i]}")

# ë°ì´í„°ì…‹ ê¸¸ì´ í™•ì¸
print(f"ì´ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")

# í† í¬ë‚˜ì´ì§• ê²°ê³¼ í™•ì¸
try:
    first_sample = dataset[0]
    print(f"\nInput IDs ê¸¸ì´ í™•ì¸: {len(first_sample['input_ids'])}")
    print(f"Attention mask ê¸¸ì´ í™•ì¸: {len(first_sample['attention_mask'])}")
    print(f"Labels íƒ€ì…: {type(first_sample['labels'])}, ê°’: {first_sample['labels']}")
except Exception as e:
    print(f"í† í¬ë‚˜ì´ì§• ê²°ê³¼ í™•ì¸ ì‹¤íŒ¨: {e}")

# ë°ì´í„°ì…‹ í˜•íƒœ ìµœì¢… ì ê²€
print(f"\në°ì´í„°ì…‹ features: {dataset.features}")

# âœ… 44ë²ˆ (í—ˆë¬´)ë§Œ í•™ìŠµí•˜ë„ë¡ íŒŒë¼ë¯¸í„° ì„ íƒ
def get_trainable_params_only_void():
    target_params = []
    for name, param in model.named_parameters():
        if "classifier.out_proj.weight" in name:
            # 44ë²ˆì§¸ weightë§Œ í•™ìŠµ
            sliced = torch.nn.Parameter(param.data[44:45])
            target_params.append({"params": sliced})
        elif "classifier.out_proj.bias" in name:
            # 44ë²ˆì§¸ biasë§Œ í•™ìŠµ
            sliced = torch.nn.Parameter(param.data[44:45])
            target_params.append({"params": sliced})
    return target_params
# 6. Few-shot Learningì„ ìœ„í•œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ
class FewShotTrainer(Trainer):
    def __init__(self, *args, old_model_state=None, regularization_weight=0.1, **kwargs):
        super().__init__(*args, **kwargs)
        self.old_model_state = old_model_state
        self.regularization_weight = regularization_weight
        # GPU device ì •ë³´ ì €ì¥
        self.device = next(self.model.parameters()).device
        print(f"ğŸ® Trainerê°€ {self.device}ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        íŒŒë©¸ì  ë§ê° ë°©ì§€ë¥¼ ìœ„í•œ ì •ê·œí™”ëœ ì†ì‹¤ í•¨ìˆ˜
        ëª¨ë¸ì˜ ê¸°ë³¸ loss ê³„ì‚°ì„ ìš°íšŒí•˜ì—¬ ì§ì ‘ ê³„ì‚°
        """
        # ğŸ® ì…ë ¥ ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                inputs[key] = value.to(model.device)

        # num_items_in_batch íŒŒë¼ë¯¸í„° ì²˜ë¦¬ (ìµœì‹  ë²„ì „ ëŒ€ì‘)
        num_items_in_batch = kwargs.get('num_items_in_batch', None)

        # ë¼ë²¨ ì¶”ì¶œ
        labels = inputs.get("labels")

        # ëª¨ë¸ì˜ ê¸°ë³¸ loss ê³„ì‚°ì„ ìš°íšŒí•˜ê¸° ìœ„í•´ labels ì œê±°
        inputs_no_labels = {k: v for k, v in inputs.items() if k != "labels"}

        # ëª¨ë¸ forward (loss ê³„ì‚° ì—†ì´)
        outputs = model(**inputs_no_labels)
        logits = outputs.logits

        # ì§ì ‘ loss ê³„ì‚°
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            # ì˜¬ë°”ë¥¸ ì°¨ì› ì‚¬ìš©: logitsì˜ ë§ˆì§€ë§‰ ì°¨ì› (45)
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
        else:
            loss = None

        # outputs ê°ì²´ì— loss ì¶”ê°€
        outputs.loss = loss

        # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ì˜ ì°¨ì´ì— ëŒ€í•œ ì •ê·œí™” í•­ ì¶”ê°€ (íŒŒë©¸ì  ë§ê° ë°©ì§€)
        if self.old_model_state is not None and self.regularization_weight > 0 and loss is not None:
            reg_loss = 0
            reg_count = 0

            for name, param in model.named_parameters():
                if name in self.old_model_state and 'classifier.out_proj.weight' in name:
                    # ê¸°ì¡´ 44ê°œ ë¼ë²¨ì˜ ê°€ì¤‘ì¹˜ë§Œ ì •ê·œí™”
                    old_weight = self.old_model_state[name][:44].to(param.device)  # GPUë¡œ ì´ë™
                    new_weight = param[:44]
                    reg_loss += F.mse_loss(new_weight, old_weight)
                    reg_count += 1

            if reg_count > 0:
                total_reg_loss = self.regularization_weight * reg_loss
                loss += total_reg_loss
                outputs.loss = loss  # outputs ì—…ë°ì´íŠ¸

                # ë¡œê¹… (ê°€ë”ì”©ë§Œ)
                if hasattr(self, '_step_count'):
                    self._step_count += 1
                else:
                    self._step_count = 1

                if self._step_count % 50 == 0:  # 50ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
                    print(f"ğŸ“Š Step {self._step_count}: Loss={loss:.4f}, Reg_loss={total_reg_loss:.4f}")

        return (loss, outputs) if return_outputs else loss


# 7. ê¸°ì¡´ ëª¨ë¸ ìƒíƒœ ì €ì¥ (ì •ê·œí™”ìš©) - ìƒˆë¡œ ìƒì„±ëœ ëª¨ë¸ ê¸°ì¤€
print("ğŸ’¾ ìƒˆ ëª¨ë¸ì˜ ê¸°ë³¸ ìƒíƒœ ì €ì¥ (ì •ê·œí™”ìš©)...")
old_model_state = {}
for name, param in model.named_parameters():
    old_model_state[name] = param.data.clone().cpu()  # CPUì— ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

print(f"âœ… {len(old_model_state)}ê°œ íŒŒë¼ë¯¸í„° ìƒíƒœ ì €ì¥ ì™„ë£Œ")

# 8. í•™ìŠµ ì„¤ì • (Few-shot Learningì— ìµœì í™”) - GPU ì‚¬ìš© ê³ ë ¤
training_args = TrainingArguments(
    output_dir="./results_void",
    evaluation_strategy="no",
    logging_steps=5,
    learning_rate=1e-5,  # ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ
    per_device_train_batch_size=4,  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥
    num_train_epochs=10,  # ë” ë§ì€ ì—í¬í¬
    weight_decay=0.01,
    logging_dir="./logs",
    save_steps=50,
    warmup_steps=10,  # Warmup ì¶”ê°€
    gradient_accumulation_steps=2,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
    dataloader_drop_last=False,
    remove_unused_columns=True,
    dataloader_pin_memory=True if torch.cuda.is_available() else False,  # GPU ì‚¬ìš©ì‹œ ë©”ëª¨ë¦¬ í•€
    ignore_data_skip=True,  # ë°ì´í„° ìŠ¤í‚µ ì˜¤ë¥˜ ë°©ì§€
    ddp_find_unused_parameters=False,  # DDP ê´€ë ¨ ì•ˆì •ì„±
    report_to=[],  # wandb ë“± ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”
    fp16=False,  # GPU ì‚¬ìš© ì‹œ mixed precision í™œì„±í™”
    torch_compile=False,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
)

print(f"ğŸ® í•™ìŠµ ì„¤ì •:")
print(f"  - ë””ë°”ì´ìŠ¤: {device}")
print(f"  - Mixed Precision (FP16): {training_args.fp16}")
print(f"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
print(f"  - ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {training_args.gradient_accumulation_steps}")

# 9. Few-shot íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = FewShotTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    old_model_state=old_model_state,
    regularization_weight=0.1,
    optimizers=(torch.optim.AdamW(get_trainable_params_only_void(), lr=1e-5), None)
)

# 10. í•™ìŠµ ì „ ëª¨ë¸ ìƒíƒœ ì¶œë ¥
print("\nğŸ” í•™ìŠµ ì „ ìµœì¢… ëª¨ë¸ ì •ë³´:")
print(f"ğŸ”¢ ì¶œë ¥ì¸µ: {model.classifier}")
print(f"ğŸ¯ Configì˜ ë¼ë²¨ ìˆ˜: {model.config.num_labels}")
print(f"ğŸ†” Modelì˜ num_labels: {getattr(model, 'num_labels', 'None')}")
print(f"ğŸ® ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
if hasattr(model.classifier, 'out_proj'):
    print(f"ğŸ“Š ìƒˆ ë¼ë²¨ ì´ˆê¸° ê°€ì¤‘ì¹˜ norm: {model.classifier.out_proj.weight.data[44].norm():.4f}")

# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (GPU ì‚¬ìš© ì‹œ)
if torch.cuda.is_available():
    print(f"ğŸ”‹ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    print(f"  - í• ë‹¹ëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024 ** 3:.2f}GB")
    print(f"  - ìºì‹œëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_reserved() / 1024 ** 3:.2f}GB")

# 11. íŒŒì¸íŠœë‹ ì‹œì‘
print("\nğŸš€ Few-shot Learning ê¸°ë°˜ íŒŒì¸íŠœë‹ ì‹œì‘...")
trainer.train()

# 12. í•™ìŠµ í›„ ê²€ì¦
print("\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ê²€ì¦ ì¤‘...")

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ - GPU ì‚¬ìš© ê³ ë ¤
test_sentences = [
    "ëª¨ë“  ê²Œ ë¬´ì˜ë¯¸í•˜ê²Œ ëŠê»´ì§„ë‹¤.",  # í—ˆë¬´
    "ì •ë§ ê¸°ì˜ê³  í–‰ë³µí•˜ë‹¤.",  # ê¸°ì¨
    "ë„ˆë¬´ í™”ê°€ ë‚œë‹¤.",  # í™”ë‚¨
    "ì‚¶ì— ì˜ë¯¸ê°€ ìˆì„ê¹Œ ì‹¶ë‹¤."  # í—ˆë¬´
]

model.eval()
with torch.no_grad():
    for sentence in test_sentences:
        # ğŸ® ì…ë ¥ì„ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True, max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}  # GPUë¡œ ì´ë™

        outputs = model(**inputs)
        predicted_id = torch.argmax(outputs.logits, dim=-1).item()
        confidence = torch.softmax(outputs.logits, dim=-1).max().item()

        print(f"ğŸ“ ë¬¸ì¥: '{sentence}'")
        print(f"ğŸ¯ ì˜ˆì¸¡: {labels[predicted_id]} (ì‹ ë¢°ë„: {confidence:.3f})")
        print()

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

# 13. ëª¨ë¸ ì €ì¥
save_path = "./custom_kote_with_void"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"ğŸ’¾ íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ·ï¸ ì´ {len(labels)}ê°œ ê°ì • ë¶„ë¥˜ ê°€ëŠ¥")
print(f"ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ ê°ì •: '{labels[44]}'")

# 14. ìµœì¢… ëª¨ë¸ ì •ë³´ ì¶œë ¥
print("\nğŸ“‹ ìµœì¢… ëª¨ë¸ ì •ë³´:")
print(f"- ê¸°ì¡´ ë¼ë²¨: 0~43 (44ê°œ)")
print(f"- ìƒˆ ë¼ë²¨: 44 (í—ˆë¬´)")
print(f"- ì´ ë¼ë²¨ ìˆ˜: {len(labels)}ê°œ")
print(f"- í•™ìŠµ ë°ì´í„°: {len(raw_data)}ê°œ (í—ˆë¬´ ê°ì •)")
print(f"- ì €ì¥ ìœ„ì¹˜: {save_path}")
print(f"- ëª¨ë¸ ì•„í‚¤í…ì²˜: {type(model).__name__}")
print(f"- Config num_labels: {model.config.num_labels}")
print(f"- ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device}")

# ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GPU ì‚¬ìš© ì‹œ)
if torch.cuda.is_available():
    print(f"\nğŸ”‹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    print(f"  - í• ë‹¹ëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024 ** 3:.2f}GB")
    print(f"  - ìµœëŒ€ ì‚¬ìš© ë©”ëª¨ë¦¬: {torch.cuda.max_memory_allocated() / 1024 ** 3:.2f}GB")

print("\nğŸ‰ GPU ìµœì í™”ëœ Few-shot Learning ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸ í™•ì¥ ì™„ë£Œ!")
